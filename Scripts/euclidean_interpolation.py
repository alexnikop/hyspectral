'''
Generates labels for unlabeled data by using the average spectral vector of each class.
These average spectral vector are calculated using the labeled data. The script calculates the euclidean 
distance of each unlabeled spectral vector with each class's average vector and finds the closest
class to it. Thresholds are also generated by sampling the normal distribution of minimum distances and
many ground truth maps are generated based on them.

usage:  python euclidean_interpolation.py dataset_folder save_dataset_folder save_images_folder (optional)
'''

import os
from PIL import Image
import skimage.external.tifffile as tif
import numpy as np
from utils import *
from sklearn.decomposition import PCA
import sys
import shutil

# reduce spectral dimensionality of images using pca.
# pca is trained on whole training dataset


def pca_spectral_dim(images, pca_components=30):

    # reshape images to a single array for pca fit
    images_reshaped = [np.reshape(x, [-1, x.shape[2]]) for x in images]
    image_vector = np.concatenate(images_reshaped)

    # use pca
    pca = PCA(n_components=pca_components, whiten=True)
    pca_vector = pca.fit_transform(image_vector)

    pca_images = list()

    vector_offset = 0

    # split images after pca transform
    for img in images:
        flat_dim = img.shape[0] * img.shape[1]
        img_split = pca_vector[vector_offset:vector_offset + flat_dim]
        img_split_reshaped = np.reshape(
            img_split, (img.shape[0], img.shape[1], pca_components))

        pca_images.append(img_split_reshaped)
        vector_offset += flat_dim

    return pca_images


# calculate each class average spectral vector by iterating
# the labeled train data


def calculate_class_reps(images, labels):

    class_dict = dict()

    for img, lab in zip(images, labels):
        for x in range(img.shape[0]):
            for y in range(img.shape[1]):
                if lab[x][y] > 0:
                    if lab[x][y] not in class_dict.keys():
                        class_dict[lab[x][y]] = list()

                    class_dict[lab[x][y]].append(img[x][y])

    for k, v in class_dict.items():
        class_dict[k] = np.mean(v, axis=0)

    return class_dict


# calculate closest class reprresentative for
# a given vector


def calculate_closest_rep(class_rep_dict, vector):
    minkey = 0
    mindist = 0

    for k, v in class_rep_dict.items():
        dist = np.linalg.norm(vector - v)

        if minkey == 0 or dist < mindist:
            minkey = k
            mindist = dist

    return minkey, mindist


# create different ground truth maps based on normal
# distribution thresholds


def create_filter_layers(labs_list,
                         lab_images_list,
                         lab_dists_list,
                         mindist_mean,
                         mindist_std,
                         thres_step_div_factor=8):

    # choose filter thresholds by sampling normal distribution

    filter_thresholds = list()

    thres_value = mindist_mean - mindist_std / 2

    while len(filter_thresholds) < 14:
        filter_thresholds.append(thres_value)
        thres_value -= mindist_std / thres_step_div_factor

    filter_layers = list()

    # generate ground truths and visualization images for each threshold

    for lab, img, dist in zip(labs_list, lab_images_list, lab_dists_list):

        image_layers = list()

        for thres in filter_thresholds:
            temp_lab = lab.copy()
            temp_img = img.copy()
            temp_lab[dist >= thres] = 0
            temp_img[dist >= thres] = [0, 0, 0]

            image_layers.append((temp_lab, temp_img, thres))

        filter_layers.append(image_layers)

    return filter_layers


# interpolate labels by finding the nearest class representative vector of
# each unlabeled spectral vector


def interpole_labels(images, orig_labels, class_rep_dict, color_dict):

    labs_list = list()
    lab_images_list = list()
    lab_dists_list = list()
    mindist_list = list()

    for img, lab in zip(images, orig_labels):

        lab_new = np.zeros((img.shape[0], img.shape[1]))
        lab_image = np.zeros((img.shape[0], img.shape[1], 3))
        lab_dist = np.zeros((img.shape[0], img.shape[1]))

        for x in range(img.shape[0]):
            for y in range(img.shape[1]):

                # calculate closest class for unlabeled data

                if lab[x][y] > 0:
                    minkey = lab[x][y]
                    mindist = 0
                else:
                    minkey, mindist = calculate_closest_rep(
                        class_rep_dict, img[x][y])
                    mindist_list.append(mindist)

                chosen_color = color_dict[minkey - 1]

                lab_new[x][y] = minkey
                lab_image[x][y] = chosen_color
                lab_dist[x][y] = mindist

        labs_list.append(lab_new.astype(np.uint8))
        lab_images_list.append(lab_image.astype(np.uint8))
        lab_dists_list.append(lab_dist)

    mindist_mean = np.mean(mindist_list)
    mindist_std = np.std(mindist_list)

    # create ground truth maps based on filter thresholds
    filter_layers = create_filter_layers(labs_list, lab_images_list,
                                         lab_dists_list, mindist_mean,
                                         mindist_std)

    return filter_layers


if __name__ == '__main__':

    if len(sys.argv) < 3:
        print(
            "pass [dataset_folder], [save_dataset_folder], [save_images_folder] (optinl) as arguments"
        )
        exit()

    dataset_path = sys.argv[1]
    train_dataset_path = os.path.join(dataset_path, 'TrainingSet')
    val_dataset_path = os.path.join(dataset_path, 'ValidationSet')
    save_dataset_path = sys.argv[2]

    create_folder(save_dataset_path)

    if len(sys.argv) > 3:
        save_images_path = sys.argv[3]
        create_folder(save_images_path)

    # load dataset images and labels
    train_data = os.listdir(train_dataset_path)
    train_images_paths = [x for x in train_data if 'GT' not in x]
    train_label_paths = [x for x in train_data if 'GT' in x]

    train_images = [
        tif.imread(os.path.join(train_dataset_path, x))
        for x in train_images_paths
    ]

    train_labels = [
        tif.imread(os.path.join(train_dataset_path, x))
        for x in train_label_paths
    ]

    # reduce spectral dimension with pca
    pca_images = pca_spectral_dim(train_images)

    # calculate class representatives
    class_rep_dict = calculate_class_reps(pca_images, train_labels)

    # get color dict to paint new labels images
    color_dict = get_color_dict()

    # interpolate images on multiple thresholds
    filter_layers = interpole_labels(pca_images, train_labels, class_rep_dict,
                                     color_dict)

    for img_idx, (image_layers, img_path, lab_path) in enumerate(
            zip(filter_layers, train_images_paths, train_label_paths)):


        for layer_data in image_layers:

            # label vector, label image, threshold value
            lab, lab_img, thres = layer_data

            # save ground truth data and visualization images
            thres_save_path = '{}/{:.1f}'.format(save_dataset_path, thres)

            if img_idx == 0:
                create_folder(thres_save_path)
                create_folder(os.path.join(thres_save_path, 'TrainingSet'))


            tif.imsave('{}/TrainingSet/{}'.format(thres_save_path, lab_path),
                       lab)
            
            shutil.copy(
                os.path.join(train_dataset_path, img_path),
                os.path.join('{}/TrainingSet'.format(thres_save_path),
                             img_path))

            if len(sys.argv) > 3:
                pil_lab_img = Image.fromarray(lab_img, 'RGB')
                pil_lab_img.save('{}/{}_{:.1f}.png'.format(
                    save_images_path,
                    lab_path.split('.')[0], thres))
